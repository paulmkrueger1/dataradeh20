{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/dataproc/docs/tutorials/python-library-example\n",
    "# pip install --upgrade google-api-python-client\n",
    "\n",
    "import googleapiclient.discovery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "dataproc = googleapiclient.discovery.build('dataproc', 'v1')\n",
    "storage_client = storage.Client(project='manymoons-215635')\n",
    "bucket_name = 'raw-events-prod'\n",
    "bucket = storage_client.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataProc Functions to Access Hive Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_hive_query(queries, projectId='manymoons-215635', region='us-central1'):\n",
    "    \"\"\"\n",
    "    1. Formats hive queries (prepends with settings)\n",
    "    2. Submit query job to DataProc\n",
    "    \n",
    "    Return:\n",
    "      res -- <dataproc.Job> dataproc job with metadata and accessors\n",
    "    \n",
    "    \"\"\"\n",
    "    formatted_queries = []\n",
    "    for query in queries:\n",
    "        updated_query = f\"\"\"\n",
    "        SET hive.mapred.supports.subdirectories=TRUE; \n",
    "        SET mapred.input.dir.recursive=TRUE;\n",
    "        {query}\n",
    "        \"\"\"\n",
    "#         updated_query = query\n",
    "        formatted_queries.append(updated_query)\n",
    "    job = {\n",
    "        \"hiveJob\": {\n",
    "          \"queryList\": {\n",
    "            \"queries\": formatted_queries\n",
    "          }\n",
    "        },\n",
    "        'placement': {\n",
    "            'clusterName': 'hive-cluster'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    job_details = {\n",
    "        'projectId': projectId,\n",
    "        'job': job\n",
    "    }\n",
    "\n",
    "    \n",
    "    return dataproc.projects().regions().jobs() \\\n",
    "        .submit(body=job_details, projectId=projectId, region=region) \\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job(job_id, project='manymoons-215635', region='us-central1'):\n",
    "    \"\"\"\n",
    "    Waits for the job to be finished, and then returns result\n",
    "    \"\"\"\n",
    "    print('Waiting for job to finish...')\n",
    "    while True:\n",
    "        result = dataproc.projects().regions().jobs().get(\n",
    "            projectId=project,\n",
    "            region=region,\n",
    "            jobId=job_id).execute()\n",
    "        # Handle exceptions\n",
    "        if result['status']['state'] == 'ERROR':\n",
    "            raise Exception(result['status']['details'])\n",
    "        elif result['status']['state'] == 'DONE':\n",
    "            print('Job finished.')\n",
    "            return result\n",
    "# [END wait]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_output(job_id, output_bucket, project_id='manymoons-215635', cluster_id='9a01738d-b251-419d-b63e-d14dd8771999'):\n",
    "    \"\"\"Downloads the output file from Cloud Storage and returns it as a\n",
    "    string.\"\"\"\n",
    "    print('Downloading output file')\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(output_bucket)\n",
    "    output_blob = (\n",
    "        'google-cloud-dataproc-metainfo/{}/jobs/{}/driveroutput.000000000'\n",
    "        .format(cluster_id, job_id))\n",
    "    return bucket.blob(output_blob).download_as_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client(project='manymoons-215635')\n",
    "bucket = client.get_bucket('raw-events-prod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Query Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_hive(hive_query):\n",
    "    \"\"\"\n",
    "    Main query helper function\n",
    "    \"\"\"\n",
    "    if type(hive_query) != type([]):\n",
    "        query = [hive_query]\n",
    "    else:\n",
    "        query = hive_query\n",
    "    res = execute_hive_query(query)\n",
    "    jobId = res['reference']['jobId']\n",
    "    try:\n",
    "        res = wait_for_job(jobId)\n",
    "        output_bucket = res['driverOutputResourceUri'].split('//')[1].split('/')[0]\n",
    "    except:\n",
    "        print('Failed')\n",
    "        output_bucket = 'dataproc-eac45f7a-3b5a-4fa4-91ea-e1dd2252fd56-us-central1'\n",
    "    out = download_output(jobId, output_bucket)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_query(query):\n",
    "    \"\"\"\n",
    "    When making data-returning queries\n",
    "    \"\"\"\n",
    "    out = query_hive(query)\n",
    "    columns = [i.strip().replace('events.', '') for i in str(out).split('\\\\n')[13].split('|') if len(i) > 0]\n",
    "    output_data = []\n",
    "    for i in str(out).split('\\\\n')[15:-5]:\n",
    "        output_data.append([i.strip() for i in i.split('|')[1:-1]])\n",
    "    return pd.DataFrame(output_data, columns=columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output = data_query(\"\"\"\n",
    "    DESCRIBE events;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"\"\"\n",
    "    SELECT * FROM events WHERE schema >= 10 AND d1='2017-09-17-13' LIMIT 2;\n",
    "\"\"\"\n",
    "\n",
    "out = data_query(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Add a Partition\n",
    "# query = \"\"\"\n",
    "# ALTER TABLE events\n",
    "#     ADD IF NOT EXISTS PARTITION (d1='2017-09-17-13')\n",
    "#     LOCATION 'gs://raw-events-prod/data/2017/09/17/13'\n",
    "# \"\"\"\n",
    "\n",
    "# out = query_hive(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Work Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Partitions created up to 2018/4/6/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "for blob in list(set(blobs)):\n",
    "    query = \"\"\"\n",
    "        ALTER TABLE events\n",
    "        ADD IF NOT EXISTS PARTITION (d1='{}')\n",
    "        LOCATION 'gs://{}'\n",
    "    \"\"\".format(blob.split('data/')[-1].replace('/','-'), blob)\n",
    "    queries.append(query)\n",
    "#     print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = []\n",
    "for i in blobs:\n",
    "    dts.append(datetime(*[int(i) for i in i.split('data/')[1].split('/')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk Partition Creator for Historical data (Scratch Work - Dont Run this)\n",
    "# i = 1\n",
    "# # int(len(queries)/200)\n",
    "# for i in range(1, int(len(queries)/200) + 1):\n",
    "# #     print(200*i, 200*(i+1))\n",
    "# #     print(i)\n",
    "#     input_queries = queries[200*i: 200*(i+1)]\n",
    "# #     print(input_queries[0])\n",
    "#     out = execute_hive_query(input_queries, projectId='manymoons-215635', region='us-central1')\n",
    "# #     print(len(input_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"SHOW partitions events\"\n",
    "out = query_hive(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(out).split('\\\\n')[11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "ALTER TABLE events2 DROP PARTITION (d1='all');\n",
    "\n",
    "ALTER TABLE events2\n",
    "    ADD PARTITION (d1='all')\n",
    "    LOCATION 'gs://raw-events-prod/180337_2018-01-28_18/'\n",
    "\"\"\"\n",
    "\n",
    "out = query_hive(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"\"\"\n",
    "    SELECT * FROM events LIMIT 2;\n",
    "\"\"\"\n",
    "\n",
    "out = query_hive(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"\"\"\n",
    "    SELECT * FROM events WHERE schema >= 10 AND d1 >= '2017-09-17-13' LIMIT 2;\n",
    "\"\"\"\n",
    "\n",
    "df = data_query(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.event_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [i.strip().replace('events.', '') for i in str(out).split('\\\\n')[13].split('|') if len(i) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = []\n",
    "for i in str(out).split('\\\\n')[15:-5]:\n",
    "    output_data.append([i.strip() for i in i.split('|')[1:-1]])\n",
    "# [i.strip() for i in str(out).split('\\\\n')[15:-5][0].split('|')[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns), len(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(output_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_in = \"\\nDROP TABLE IF EXISTS events;\\nCREATE EXTERNAL TABLE events\\n      (tion_time STRING, user_id STRING, user_properties STRING, uuid STRING, version_name STRING, amplitude_attribution_ids STRING, amplitude_id STRING, app STRING, event_id STRING, session_id STRING, is_attribution_event STRING)\\n      PARTITIONED BY (d1 STRING) \\n      STORED AS PARQUET\\n      LOCATION 'gs://raw-events-prod/data';\\n\"\n",
    "print(q_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_string = 'insert_id STRING, schema INT, adid STRING, amplitude_event_type STRING, amplitude_id INT, app INT, city STRING, client_event_time STRING, client_upload_time STRING, country STRING, data STRING, device_brand STRING, device_carrier STRING, device_family STRING, device_id STRING, device_manufacturer STRING, device_model STRING, device_type STRING, dma STRING, event_id INT, event_properties STRING, event_time STRING, event_type STRING, group_properties STRING, groups STRING, idfa STRING, ip_address STRING, language STRING, library STRING, location_lat STRING, location_lng STRING, os_name STRING, os_version STRING, paying STRING, platform STRING, processed_time STRING, region STRING, sample_rate STRING, server_upload_time STRING, session_id INT, start_version STRING, user_creation_time STRING, user_id STRING, user_properties STRING, uuid STRING, version_name STRING'\n",
    "que = f\"\"\"\n",
    "DROP TABLE IF EXISTS events;\n",
    "CREATE EXTERNAL TABLE events\n",
    "      ({table_string})\n",
    "      PARTITIONED BY (d1 STRING) \n",
    "      STORED AS PARQUET\n",
    "      LOCATION 'gs://raw-events-prod/data';\n",
    "\"\"\"\n",
    "que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = query_hive(que)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlenv]",
   "language": "python",
   "name": "conda-env-mlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
